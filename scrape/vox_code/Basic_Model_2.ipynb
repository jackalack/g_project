{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vox_articles_to_db import create_db \n",
    "\n",
    "from numpy.random import rand, RandomState\n",
    "from numpy import array, matrix, linalg\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "#Stemming and Lemmatizing packages\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stemmer = Stemmer.Stemmer('en')\n",
    "snowball = SnowballStemmer('english')      \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: english_stemmer.stemWords(analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vox_raw_df = create_db()\n",
    "vox_df = vox_raw_df[vox_raw_df[1]!='']\n",
    "vox_df.shape\n",
    "doc_bodies = vox_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reconst_mse(target, left, right):\n",
    "    return (array(target - left.dot(right))**2).mean()\n",
    "\n",
    "def describe_nmf_results(document_term_mat, W, H, n_top_words = 20):\n",
    "    \n",
    "    print(\"Reconstruction error: %f\") %(reconst_mse(document_term_mat, W, H))\n",
    "    topics = []\n",
    "    for topic_num, topic in enumerate(H):\n",
    "        curr_topic = [feature_words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(curr_topic)\n",
    "        print(\"Topic %d:\" % topic_num)\n",
    "        print(\" \".join(curr_topic))   \n",
    "    return topics\n",
    "\n",
    "\n",
    "def calc_cosine_similarity(word_matrix, topic_vector):\n",
    "    cosine_similarities = [1 - spatial.distance.cosine(article.todense(), topic_vector) for article in word_matrix]\n",
    "    return cosine_similarities\n",
    "\n",
    "def calc_average_cosine_similarity(H_matrix):\n",
    "    print H_matrix.shape\n",
    "    h_2 = H_matrix\n",
    "    indices = [0,1,2,3,4]\n",
    "    similarities = [calc_cosine_similarity_topics(h_2, primary_topic, index) for primary_topic, index in zip(H_matrix, indices)]\n",
    "    averages = [reduce(lambda x, y: x + y, topic) / len(topic) for topic in similarities]\n",
    "    return averages\n",
    "\n",
    "def calc_cosine_similarity_topics(H_matrix, primary_topic, index):\n",
    "    print primary_topic.shape\n",
    "    results = [1 - spatial.distance.cosine(primary_topic, topic) for topic in H_matrix]\n",
    "    results.pop(index)\n",
    "    print results\n",
    "    return results\n",
    "\n",
    "def convert_corpus_to_matrix(doc_bodies, n_features = 5000):\n",
    "    vectorizer = StemmedTfidfVectorizer(max_features=n_features, stop_words='english', analyzer='word', ngram_range=(1,1))\n",
    "    document_term_mat = vectorizer.fit_transform(doc_bodies)\n",
    "    return vectorizer, document_term_mat\n",
    "\n",
    "def calculate_word_dictionary(vectorizer):\n",
    "    word_dictionary  = {}\n",
    "    for index, word in enumerate(vectorizer.get_feature_names()):\n",
    "        word_dictionary[word] = index \n",
    "    return word_dictionary\n",
    "\n",
    "def generate_latent_topics(subset_matrix, tolerance, n_components = 5):\n",
    "    print(\"\\n\\n---------\\nsklearn decomposition\")\n",
    "    disimilar = False\n",
    "    while not disimilar:\n",
    "        print \"check\"\n",
    "        nmf = NMF(n_components,tol=tolerance)\n",
    "        W_sklearn = nmf.fit_transform(subset_matrix)\n",
    "        H_sklearn = nmf.components_\n",
    "        averages = calc_average_cosine_similarity(H_sklearn)\n",
    "        print averages\n",
    "        disimilar = all(i <= .45 for i in averages)\n",
    "        n_components -= 1  \n",
    "    return W_sklearn, H_sklearn\n",
    "\n",
    "def generate_latent_topics_2(subset_matrix, tolerance, n_components = 5):\n",
    "    print(\"\\n\\n---------\\nsklearn decomposition\")\n",
    "    nmf = NMF(n_components,tol=tolerance)\n",
    "    W_sklearn = nmf.fit_transform(subset_matrix)\n",
    "    H_sklearn = nmf.components_\n",
    "    return W_sklearn, H_sklearn\n",
    "\n",
    "#find indices of documents with consine similarity > threshold \n",
    "#                  *** threshold needs to be more intentional ***\n",
    "def create_matrix_subset(raw_query, document_term_mat, threshold):\n",
    "    vectorized_query = vectorizer.transform([raw_query])\n",
    "    results = calc_cosine_similarity(document_term_mat, vectorized_query.todense())\n",
    "    num_positive_values = len([result for result in results if result > threshold])\n",
    "    positive_indices = np.argsort(np.abs(results))[-num_positive_values:-1]\n",
    "    return document_term_mat[positive_indices]\n",
    "\n",
    "def segment_and_categorize(query, current_matrix, tolerance, threshold= .05, n_components = 5):\n",
    "    matrix_subset = create_matrix_subset(query,current_matrix, threshold)\n",
    "    W_sklearn, H_sklearn = generate_latent_topics(matrix_subset, tolerance, n_components)\n",
    "    topics = describe_nmf_results(matrix_subset, W_sklearn, H_sklearn)\n",
    "    return matrix_subset, topics\n",
    "#find indices of documents with consine similarity > threshold \n",
    "#                  *** threshold needs to be more intentional ***\n",
    "def create_matrix_subset(raw_query, document_term_mat, threshold):\n",
    "    vectorized_query = vectorizer.transform([raw_query])\n",
    "    results = calc_cosine_similarity(document_term_mat, vectorized_query.todense())\n",
    "    num_positive_values = len([result for result in results if result > threshold])\n",
    "    positive_indices = np.argsort(np.abs(results))[-num_positive_values:-1]\n",
    "    return document_term_mat[positive_indices]\n",
    "\n",
    "def segment_and_categorize(query, current_matrix, tolerance, threshold= .05, n_components = 5):\n",
    "    matrix_subset = create_matrix_subset(query,current_matrix, threshold)\n",
    "    W_sklearn, H_sklearn = generate_latent_topics(matrix_subset, tolerance, n_components)\n",
    "    topics = describe_nmf_results(matrix_subset, W_sklearn, H_sklearn)\n",
    "    return matrix_subset, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "check\n",
      "(5, 5000)\n",
      "(5000,)\n",
      "[0.31951504392442787, 0.24453949640072226, 0.13849699886340561, 0.39554392358508084]\n",
      "(5000,)\n",
      "[0.31951504392442787, 0.20511430042692536, 0.19228475435750148, 0.33757848111889877]\n",
      "(5000,)\n",
      "[0.24453949640072226, 0.20511430042692536, 0.15064274290686852, 0.24005581905307416]\n",
      "(5000,)\n",
      "[0.13849699886340561, 0.19228475435750148, 0.15064274290686852, 0.20710525938894864]\n",
      "(5000,)\n",
      "[0.39554392358508084, 0.33757848111889877, 0.24005581905307416, 0.20710525938894864]\n",
      "[0.27452386569340914, 0.26362314495693839, 0.21008808969689757, 0.17213243887918106, 0.29507087078650063]\n",
      "Reconstruction error: 0.000185\n",
      "Topic 0:\n",
      "season charact film episod movi like stori seri just book time tv peopl make thing women watch way men play\n",
      "Topic 1:\n",
      "republican obama democrat clinton immigr presid parti trump senat polit bush vote elect iran candid campaign polici isi poll hous\n",
      "Topic 2:\n",
      "polic black offic ferguson protest shoot brown kill depart report justic arrest racial white crime crimin gun peopl forc cop\n",
      "Topic 3:\n",
      "court marriag sex marijuana state suprem law legal rule feder ban decis lgbt drug case gay discrimin right equal coupl\n",
      "Topic 4:\n",
      "health percent peopl insur care ebola tax rate student year state price colleg incom research cost obamacar patient studi spend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[u'season',\n",
       "  u'charact',\n",
       "  u'film',\n",
       "  u'episod',\n",
       "  u'movi',\n",
       "  u'like',\n",
       "  u'stori',\n",
       "  u'seri',\n",
       "  u'just',\n",
       "  u'book',\n",
       "  u'time',\n",
       "  u'tv',\n",
       "  u'peopl',\n",
       "  u'make',\n",
       "  u'thing',\n",
       "  u'women',\n",
       "  u'watch',\n",
       "  u'way',\n",
       "  u'men',\n",
       "  u'play'],\n",
       " [u'republican',\n",
       "  u'obama',\n",
       "  u'democrat',\n",
       "  u'clinton',\n",
       "  u'immigr',\n",
       "  u'presid',\n",
       "  u'parti',\n",
       "  u'trump',\n",
       "  u'senat',\n",
       "  u'polit',\n",
       "  u'bush',\n",
       "  u'vote',\n",
       "  u'elect',\n",
       "  u'iran',\n",
       "  u'candid',\n",
       "  u'campaign',\n",
       "  u'polici',\n",
       "  u'isi',\n",
       "  u'poll',\n",
       "  u'hous'],\n",
       " [u'polic',\n",
       "  u'black',\n",
       "  u'offic',\n",
       "  u'ferguson',\n",
       "  u'protest',\n",
       "  u'shoot',\n",
       "  u'brown',\n",
       "  u'kill',\n",
       "  u'depart',\n",
       "  u'report',\n",
       "  u'justic',\n",
       "  u'arrest',\n",
       "  u'racial',\n",
       "  u'white',\n",
       "  u'crime',\n",
       "  u'crimin',\n",
       "  u'gun',\n",
       "  u'peopl',\n",
       "  u'forc',\n",
       "  u'cop'],\n",
       " [u'court',\n",
       "  u'marriag',\n",
       "  u'sex',\n",
       "  u'marijuana',\n",
       "  u'state',\n",
       "  u'suprem',\n",
       "  u'law',\n",
       "  u'legal',\n",
       "  u'rule',\n",
       "  u'feder',\n",
       "  u'ban',\n",
       "  u'decis',\n",
       "  u'lgbt',\n",
       "  u'drug',\n",
       "  u'case',\n",
       "  u'gay',\n",
       "  u'discrimin',\n",
       "  u'right',\n",
       "  u'equal',\n",
       "  u'coupl'],\n",
       " [u'health',\n",
       "  u'percent',\n",
       "  u'peopl',\n",
       "  u'insur',\n",
       "  u'care',\n",
       "  u'ebola',\n",
       "  u'tax',\n",
       "  u'rate',\n",
       "  u'student',\n",
       "  u'year',\n",
       "  u'state',\n",
       "  u'price',\n",
       "  u'colleg',\n",
       "  u'incom',\n",
       "  u'research',\n",
       "  u'cost',\n",
       "  u'obamacar',\n",
       "  u'patient',\n",
       "  u'studi',\n",
       "  u'spend']]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tolerance = .01\n",
    "vectorizer, document_term_mat = convert_corpus_to_matrix(doc_bodies)\n",
    "feature_words = vectorizer.get_feature_names()\n",
    "word_dictionary = calculate_word_dictionary(vectorizer)\n",
    "W_sklearn, H_sklearn = generate_latent_topics(document_term_mat, tolerance)\n",
    "describe_nmf_results(document_term_mat, W_sklearn, H_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "check\n",
      "(5, 5000)\n",
      "(5000,)\n",
      "[0.12422366705130061, 0.11505251886581225, 0.182773512588704, 0.39197911298165511]\n",
      "(5000,)\n",
      "[0.12422366705130061, 0.045055348037094989, 0.051310028623774984, 0.20542477057389452]\n",
      "(5000,)\n",
      "[0.11505251886581225, 0.045055348037094989, 0.22046071137272305, 0.31614009528789122]\n",
      "(5000,)\n",
      "[0.182773512588704, 0.051310028623774984, 0.22046071137272305, 0.30667965291524701]\n",
      "(5000,)\n",
      "[0.39197911298165511, 0.20542477057389452, 0.31614009528789122, 0.30667965291524701]\n",
      "[0.20350720287186799, 0.10650345357151628, 0.17417716839088038, 0.19030597637511226, 0.30505590793967197]\n",
      "Reconstruction error: 0.000138\n",
      "Topic 0:\n",
      "stock chines market china margin trade govern price economi invest borrow compani econom growth investor index percent money crash buy\n",
      "Topic 1:\n",
      "hong kong protest beij chines china democraci polic crackdown govern communist movement protestor 1989 pro demonstr unrest freedom student peac\n",
      "Topic 2:\n",
      "emiss climat carbon global coal warm energi china countri india 2c pledg dioxid gas world cut chang plant percent greenhous\n",
      "Topic 3:\n",
      "tpp trade deal iran agreement obama countri russia negoti sanction china polici america american administr congress foreign say nuclear econom\n",
      "Topic 4:\n",
      "china chines countri world japan korea beij russia state north econom growth economi govern unit india year militari oil american\n",
      "(261, 5000)\n"
     ]
    }
   ],
   "source": [
    "main_query = 'China'\n",
    "subset_matrix = document_term_mat\n",
    "\n",
    "new_matrix, new_topics = segment_and_categorize(main_query, subset_matrix, tolerance)\n",
    "print new_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "check\n",
      "(5, 5000)\n",
      "(5000,)\n",
      "[0.39324304130873733, 0.66757581885351702, 0.301153300270944, 0.43441995381396192]\n",
      "(5000,)\n",
      "[0.39324304130873733, 0.35209789101001854, 0.4036091244827491, 0.37967802025576669]\n",
      "(5000,)\n",
      "[0.66757581885351702, 0.35209789101001854, 0.3262168205112348, 0.30735850475594573]\n",
      "(5000,)\n",
      "[0.301153300270944, 0.4036091244827491, 0.3262168205112348, 0.24609207303287839]\n",
      "(5000,)\n",
      "[0.43441995381396192, 0.37967802025576669, 0.30735850475594573, 0.24609207303287839]\n",
      "[0.4490980285617901, 0.38215701926431789, 0.41331225878267902, 0.31926782957445154, 0.34188713796463821]\n",
      "Reconstruction error: 0.000027\n",
      "Topic 0:\n",
      "stock market chines price govern china economi share declin econom investor peopl invest borrow crash boom fall week compani percent\n",
      "Topic 1:\n",
      "china chines economi growth econom invest market consumpt polit model reform countri crash elit export communist driven problem transit realli\n",
      "Topic 2:\n",
      "stock margin chines trade market china borrow percent invest investor june money compani leverag index financ composit wednesday govern announc\n",
      "Topic 3:\n",
      "debt bond buy govern chines china japan dollar japanes fund borrow invest export feder foreign need currenc financ trust buyer\n",
      "Topic 4:\n",
      "japan 1990 stock crash market growth asian economi econom chines index soar 000 american export china decad larg necessarili bad\n",
      "(36, 5000)\n"
     ]
    }
   ],
   "source": [
    "new_query = main_query + ' '.join(new_topics[0])\n",
    "new_matrix, new_topics = segment_and_categorize(new_query, new_matrix, tolerance, threshold = .2)\n",
    "print new_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "check\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-21d3446a8aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_query\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_and_categorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnew_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-ce4c804745c4>\u001b[0m in \u001b[0;36msegment_and_categorize\u001b[0;34m(query, current_matrix, tolerance, threshold, n_components)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msegment_and_categorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmatrix_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_matrix_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mW_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescribe_nmf_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_sklearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-ce4c804745c4>\u001b[0m in \u001b[0;36mgenerate_latent_topics\u001b[0;34m(subset_matrix, tolerance, n_components)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"check\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mW_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mH_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0maverages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_average_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_sklearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# update H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_H\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miterH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mtolH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtolH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_update_H\u001b[0;34m(self, X, H, W, tolH)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparseness\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             H, gradH, iterH = _nls_subproblem(X, W, H, tolH,\n\u001b[0;32m--> 443\u001b[0;31m                                               self.nls_max_iter)\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparseness\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             H, gradH, iterH = _nls_subproblem(\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_nls_subproblem\u001b[0;34m(V, W, H, tol, max_iter, sigma, beta)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mgradd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mdQd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWtW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0msuff_decr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdQd\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minner_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_query += ' '.join(new_topics[2])\n",
    "new_matrix, new_topics = segment_and_categorize(new_query, new_matrix, tolerance, threshold = .3)\n",
    "print new_matrix.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "check\n",
      "(5, 5000)\n",
      "(5000,)\n",
      "[0.45853811071760586, 0.39543875899215042, 0.44131807699232173, 0.42641500163915846]\n",
      "(5000,)\n",
      "[0.45853811071760586, 0.52497786825997539, 0.51101730709077253, 0.40476792647590487]\n",
      "(5000,)\n",
      "[0.39543875899215042, 0.52497786825997539, 0.41434156057731408, 0.46751363039769744]\n",
      "(5000,)\n",
      "[0.44131807699232173, 0.51101730709077253, 0.41434156057731408, 0.37876651746795065]\n",
      "(5000,)\n",
      "[0.42641500163915846, 0.40476792647590487, 0.46751363039769744, 0.37876651746795065]\n",
      "[0.43042748708530909, 0.47482530313606464, 0.45056795455678433, 0.43636086553208975, 0.41936576899517786]\n",
      "Reconstruction error: 0.000029\n",
      "Topic 0:\n",
      "enrol healthcar gov obamacar coverag million sign year peopl open shopper plan insur administr select health marketplac launch consum applic\n",
      "Topic 1:\n",
      "insur obamacar health enrolle sign exchang predict coverag peopl care law premium data fix octob gov million up healthcar expans\n",
      "Topic 2:\n",
      "premium shopper insur plan marketplac health obamacar coverag averag healthcar cost competit lower satisfact price subsidi feder pay gov subsid\n",
      "Topic 3:\n",
      "subsidi insur exchang coverag obamacar healthcar marketplac gov court robert state afford feder act congress health enrol care suprem washington\n",
      "Topic 4:\n",
      "plan cruz healthcar gov insur coverag health famili shop emerg care option doctor premium charg 000 network cost deduct structur\n",
      "(30, 5000)\n"
     ]
    }
   ],
   "source": [
    "new_query += ' '.join(new_topics[4])\n",
    "new_matrix, new_topics = segment_and_categorize(new_query, new_matrix, tolerance, threshold = .4)\n",
    "print new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "check\n",
      "(5, 5000)\n",
      "(5000,)\n",
      "[0.31351396065816428, nan, nan, nan]\n",
      "(5000,)\n",
      "[0.31351396065816428, nan, nan, nan]\n",
      "(5000,)\n",
      "[nan, nan, nan, nan]\n",
      "(5000,)\n",
      "[nan, nan, nan, nan]\n",
      "(5000,)\n",
      "[nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "check\n",
      "(4, 5000)\n",
      "(5000,)\n",
      "[0.31351396065816417, nan, nan]\n",
      "(5000,)\n",
      "[0.31351396065816417, nan, nan]\n",
      "(5000,)\n",
      "[nan, nan, nan]\n",
      "(5000,)\n",
      "[nan, nan, nan]\n",
      "[nan, nan, nan, nan]\n",
      "check\n",
      "(3, 5000)\n",
      "(5000,)\n",
      "[0.31351396065816406, nan]\n",
      "(5000,)\n",
      "[0.31351396065816406, nan]\n",
      "(5000,)\n",
      "[nan, nan]\n",
      "[nan, nan, nan]\n",
      "check\n",
      "(2, 5000)\n",
      "(5000,)\n",
      "[0.31351396065816428]\n",
      "(5000,)\n",
      "[0.31351396065816428]\n",
      "[0.31351396065816428, 0.31351396065816428]\n",
      "Reconstruction error: 0.000000\n",
      "Topic 0:\n",
      "cruz healthcar plan gov insur famili emerg shop coverag health care option doctor obamacar charg premium 000 network cost deduct\n",
      "Topic 1:\n",
      "shopper insur subsidi subsid marketplac competit averag premium health plan pay generous month obamacar healthcar feder report cost price low\n",
      "(15, 5000)\n"
     ]
    }
   ],
   "source": [
    "new_query += ' '.join(new_topics[2])\n",
    "new_matrix, new_topics = segment_and_categorize(new_query, new_matrix, tolerance, threshold = .4)\n",
    "print new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_latent_topics_2(subset_matrix, n_components = 5):\n",
    "    print(\"\\n\\n---------\\nsklearn decomposition\")\n",
    "    nmf = NMF(n_components)\n",
    "    W_sklearn = nmf.fit_transform(subset_matrix)\n",
    "    H_sklearn = nmf.components_\n",
    "    return W_sklearn, H_sklearn\n",
    "\n",
    "def calc_cosine_similarity_2(word_matrix, topic_vector):\n",
    "    cosine_similarities = [1 - spatial.distance.cosine(article.todense(), topic_vector) for article in word_matrix]\n",
    "    return cosine_similarities\n",
    "\n",
    "#find indices of documents with consine similarity > threshold \n",
    "#                  *** threshold needs to be more intentional ***\n",
    "def create_matrix_subset_2(raw_query, document_term_mat, threshold):\n",
    "    results = calc_cosine_similarity(document_term_mat, raw_query.todense())\n",
    "    num_positive_values = len([result for result in results if result > threshold])\n",
    "    positive_indices = np.argsort(np.abs(results))[-num_positive_values:-1]\n",
    "    return document_term_mat[:document_term_mat.shape[0]/2]\n",
    "\n",
    "def segment_and_categorize_2(query, current_matrix, threshold= .05, n_components = 5):\n",
    "    matrix_subset = create_matrix_subset_2(query,current_matrix, threshold)\n",
    "    W_sklearn, H_sklearn = generate_latent_topics_2(matrix_subset, n_components)\n",
    "    topics = describe_nmf_results(matrix_subset, W_sklearn, H_sklearn)\n",
    "    return matrix_subset, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000185\n",
      "Topic 0:\n",
      "film season charact episod movi stori like seri just tv book time peopl make thing watch way play women best\n",
      "Topic 1:\n",
      "percent health tax insur state peopl rate year student price obamacar incom colleg cost care job market compani pay plan\n",
      "Topic 2:\n",
      "ebola health outbreak diseas patient virus hospit infect africa vaccin care liberia doctor west peopl case epidem medic leon sierra\n",
      "Topic 3:\n",
      "polic offic black ferguson shoot protest brown justic kill arrest white depart report racial gun law death crimin crime peopl\n",
      "Topic 4:\n",
      "obama republican clinton democrat immigr trump presid isi parti senat polit bush iran vote state polici campaign candid elect support\n",
      "(5343, 5000)\n",
      "\n",
      "\n",
      " Iteration  1\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000184\n",
      "Topic 0:\n",
      "season film episod charact movi stori like seri tv just book watch thing time make peopl play love men way\n",
      "Topic 1:\n",
      "obama republican immigr democrat clinton presid polit parti senat state vote isi trump bush iran polici hous campaign candid elect\n",
      "Topic 2:\n",
      "ebola health outbreak diseas virus patient infect africa west epidem care liberia vaccin leon hospit sierra doctor peopl countri case\n",
      "Topic 3:\n",
      "polic offic black ferguson protest shoot justic brown depart kill report white racial law arrest wilson death court crimin investig\n",
      "Topic 4:\n",
      "percent health insur peopl tax state price rate year student incom obamacar colleg compani market plan care cost new spend\n",
      "(2671, 5000)\n",
      "\n",
      "\n",
      " Iteration  2\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000184\n",
      "Topic 0:\n",
      "charact episod season like film stori book movi just peopl tv make women thing seri men love know realli game\n",
      "Topic 1:\n",
      "percent insur health tax state obamacar price rate incom peopl year economi plan spend care exchang feder american job new\n",
      "Topic 2:\n",
      "obama republican clinton democrat presid immigr polit parti senat hous isi bush campaign state polici vote trump elect candid israel\n",
      "Topic 3:\n",
      "polic court marriag sex law ferguson offic black justic suprem protest state rule legal shoot feder lgbt report said arrest\n",
      "Topic 4:\n",
      "ebola health diseas outbreak virus patient africa infect care hospit duncan peopl countri liberia west epidem case doctor contact spread\n",
      "(1335, 5000)\n",
      "\n",
      "\n",
      " Iteration  3\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000184\n",
      "Topic 0:\n",
      "episod charact season book like stori peopl movi just make film game thing realli love women way televis tv seri\n",
      "Topic 1:\n",
      "health percent obamacar insur state peopl ebola rate care tax incom worker compani year law poverti exchang drug feder spend\n",
      "Topic 2:\n",
      "polic offic ferguson court black justic protest law shoot report depart student kill crimin arrest said cop loui violenc death\n",
      "Topic 3:\n",
      "obama republican clinton democrat presid isi polit parti bush campaign hous senat candid israel vote trump state polici govern american\n",
      "Topic 4:\n",
      "immigr deport unauthor agent border children daca citizen obama reform parent countri patrol administr action born year nativ trump citizenship\n",
      "(667, 5000)\n",
      "\n",
      "\n",
      " Iteration  4\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000182\n",
      "Topic 0:\n",
      "insur obamacar court health state law subsidi exchang coverag suprem rule enrol care king plan feder million decis marketplac case\n",
      "Topic 1:\n",
      "polic student colleg offic black sexual assault shoot campus law live cop educ justic violenc ferguson matter protest school court\n",
      "Topic 2:\n",
      "ebola health diseas drug patient care duncan virus antibiot hospit quarantin infect resist outbreak peopl worker africa doctor emerg spread\n",
      "Topic 3:\n",
      "clinton republican obama democrat presid campaign immigr candid hous senat trump polit bush hillari vote isi presidenti polici administr parti\n",
      "Topic 4:\n",
      "peopl episod percent season like charact year make work rate women movi american job just time economi thing new class\n",
      "(333, 5000)\n",
      "\n",
      "\n",
      " Iteration  5\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000179\n",
      "Topic 0:\n",
      "episod season rate peopl work percent women make like job inflat growth time hour charact use year fed just say\n",
      "Topic 1:\n",
      "health ebola diseas care insur obamacar patient duncan emerg hospit mental polio virus percent peopl panel coverag spend million va\n",
      "Topic 2:\n",
      "immigr deport daca unauthor parent qualifi administr migrant program dreamer militari born legal obama elig citizen citizenship recipi action hous\n",
      "Topic 3:\n",
      "polic sexual shoot offic violenc muslim hebdo student assault charli crime colleg live campus law black victim gun freedom cop\n",
      "Topic 4:\n",
      "democrat republican clinton hous senat vote presid obama presidenti candid campaign hillari white polit governor court state parti walker voter\n",
      "(166, 5000)\n",
      "\n",
      "\n",
      " Iteration  6\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000173\n",
      "Topic 0:\n",
      "obamacar insur health coverag million kentucki care marketplac subsidi premium uninsur rand percent jone mother enrol gain refund newli estim\n",
      "Topic 1:\n",
      "democrat republican senat hous clinton warren vote cia parti report liber presidenti voter obama hillari candid polit presid boehner primari\n",
      "Topic 2:\n",
      "episod season charact writer throne dunham sansa girl sequenc vampir scene seri todd game week just detect ray stori watch\n",
      "Topic 3:\n",
      "immigr deport parent daca migrant dreamer administr reform obama children unauthor child relief legal kennedi program recipi qualifi protect action\n",
      "Topic 4:\n",
      "diseas 000 percent ebola inflat peopl rate oil health hebdo panel year charli isi ice make emerg work drug patient\n",
      "(83, 5000)\n",
      "\n",
      "\n",
      " Iteration  7\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000161\n",
      "Topic 0:\n",
      "clinton warren candid hillari presid walker presidenti campaign polit court voter democrat run liber media obama christi report governor china\n",
      "Topic 1:\n",
      "cia tortur report prosecut detaine intellig senat interrog ezra klein reveal summari abus utter outrag detail sentenc absolut failur 16\n",
      "Topic 2:\n",
      "episod season vampir charact ray fx hous detect crystal shoot sketch paul comedian crime strain card frank assault littl night\n",
      "Topic 3:\n",
      "emerg health mother diseas panel peopl rail reactor patient chariti donat oil mental isi percent year care capac louisiana art\n",
      "Topic 4:\n",
      "immigr migrant daca boehner hous republican reform pass vote parent democrat dreamer legal kennedi obama wheeler crimin congress children conserv\n",
      "(41, 5000)\n",
      "\n",
      "\n",
      " Iteration  8\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000142\n",
      "Topic 0:\n",
      "episod vampir fx crystal sketch charact strain comedian card comedi season tv hous like night josh jim creatur great littl\n",
      "Topic 1:\n",
      "mother jone warren daca babi subsidi levi court wheeler case insur cell obama parent milk plaintiff languag dreamer presid breast\n",
      "Topic 2:\n",
      "gas louisiana frack coal land sea shale len sink flood rise climat nigeria energi boko haram delta gulf level oil\n",
      "Topic 3:\n",
      "tweet hebdo donat charli chariti delet al diseas money report accord screenshot caus moham ice magazin cartoon french jay peopl\n",
      "Topic 4:\n",
      "insect art patent price palestinian tool manipul artist flight inequ refuge seat appeal arrang rich avail easi paint buy pin\n",
      "(20, 5000)\n",
      "\n",
      "\n",
      " Iteration  9\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000095\n",
      "Topic 0:\n",
      "art crystal card sketch nigeria comedian hous boko haram comedi price fx lesson season josh tv artist northern like think\n",
      "Topic 1:\n",
      "tweet donat delet chariti money diseas screenshot al jay gave bail caus ice complex beyonc carter impact peopl health dollar\n",
      "Topic 2:\n",
      "wheeler warren fcc neutral provis network court rule propos compani make telecommun toxic settlement taxpay tran obscur partnership investor challeng\n",
      "Topic 3:\n",
      "palestinian refuge israel isra gaza photo arab archiv camp document date conflict establish 15 14th 1979 palestin sin descend home\n",
      "Topic 4:\n",
      "patent flight seat arrang futur knee belt convey googl constant arm space food idea soda width atlanta sequel hover hip\n",
      "(10, 5000)\n",
      "\n",
      "\n",
      " Iteration  10\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n",
      "Reconstruction error: 0.000000\n",
      "Topic 0:\n",
      "tweet delet screenshot jay gave bail complex beyonc carter donat magazin confirm money remind hugh bread eras filmmak howard later\n",
      "Topic 1:\n",
      "patent flight seat arrang futur knee belt convey googl constant arm space food idea soda width atlanta sequel hover hip\n",
      "Topic 2:\n",
      "palestinian refuge israel isra gaza photo arab archiv camp document date conflict establish 15 14th 1979 palestin sin descend home\n",
      "Topic 3:\n",
      "nigeria boko haram northern nigerian muslim jonathan north think christian coloni bargain british presid ethnic south divis delta movement countri\n",
      "Topic 4:\n",
      "chariti donat diseas al caus money ice impact health peopl dollar fundrais rais develop adjust die challeng motor qualiti celebr\n",
      "(5, 5000)\n",
      "\n",
      "\n",
      " Iteration  11\n",
      "\n",
      "\n",
      "---------\n",
      "sklearn decomposition\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6e331d80ad49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n\\n Iteration \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhalf_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_and_categorize_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mhalf_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f068544b738c>\u001b[0m in \u001b[0;36msegment_and_categorize_2\u001b[0;34m(query, current_matrix, threshold, n_components)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msegment_and_categorize_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmatrix_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_matrix_subset_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mW_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_topics_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescribe_nmf_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_sklearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f068544b738c>\u001b[0m in \u001b[0;36mgenerate_latent_topics_2\u001b[0;34m(subset_matrix, n_components)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n---------\\nsklearn decomposition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mW_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mH_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mW_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         gradW = (np.dot(W, np.dot(H, H.T))\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nndsvd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_nmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nndsvda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_nmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_initialize_nmf\u001b[0;34m(X, n_components, variant, eps, random_state)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# extract positive and negative parts of column vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "random_vector = document_term_mat[1]\n",
    "half_mat, half_topics = segment_and_categorize_2(random_vector, document_term_mat)\n",
    "print half_mat.shape\n",
    "\n",
    "for x in xrange(1,20):\n",
    "    print \"\\n\\n Iteration \",x\n",
    "    half_mat, half_topics = segment_and_categorize_2(random_vector, half_mat)\n",
    "    print half_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 70 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
