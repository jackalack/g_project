{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vox_articles_to_db import create_db \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from numpy.random import rand, RandomState\n",
    "from numpy import array, matrix, linalg\n",
    "from scipy import spatial\n",
    "import math\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "#Stemming and Lemmatizing packages\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_stemmer = Stemmer.Stemmer('en')\n",
    "snowball = SnowballStemmer('english')      \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: english_stemmer.stemWords(analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vox_raw_df = create_db()\n",
    "vox_df = vox_raw_df[vox_raw_df[1]!='']\n",
    "vox_df.shape\n",
    "doc_bodies = vox_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=8, n_init=10,\n",
       "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(doc_bodies)\n",
    "features = vectorizer.get_feature_names()\n",
    "kmeans = KMeans()\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for each cluster:\n",
      "0: marijuana, legalization, drug, pot, medical, alcohol, dc, colorado, drugs, state\n",
      "1: percent, tax, students, income, people, college, economy, economic, rate, workers\n",
      "2: health, obamacare, insurance, care, medicaid, coverage, law, subsidies, states, plans\n",
      "3: ebola, outbreak, health, virus, disease, africa, liberia, leone, west, sierra\n",
      "4: sex, marriage, court, supreme, lgbt, couples, marriages, discrimination, gay, rights\n",
      "5: police, ferguson, officers, black, brown, officer, shooting, wilson, justice, protests\n",
      "6: people, like, new, just, time, obama, said, clinton, women, trump\n",
      "7: isis, iran, israel, iraq, obama, russia, nuclear, israeli, deal, syria\n"
     ]
    }
   ],
   "source": [
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "print \"top features for each cluster:\"\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_topic_descriptions(articles):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    x = vectorizer.fit_transform(articles)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    kmeans = KMeans(n_clusters = 5)\n",
    "    kmeans.fit(x)\n",
    "    sub_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "    assigned_cluster = kmeans.transform(x).argmin(axis=1)\n",
    "    for num, centroid in enumerate(sub_centroids):\n",
    "        print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))\n",
    "    return assigned_cluster\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n",
      "0: baltimore, gray, police, mosby, injury, freddie, officers, spinal, van, cord\n",
      "1: wilson, brown, grand, jury, mcculloch, police, evidence, shooting, ferguson, darren\n",
      "2: police, officers, officer, force, rice, use, garner, killed, gun, shooting\n",
      "3: police, black, people, officers, white, percent, ferguson, justice, americans, racial\n",
      "4: police, ferguson, gas, protesters, tear, st, louis, night, protests, brown\n"
     ]
    }
   ],
   "source": [
    "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
    "\n",
    "\n",
    "def sub_topics(articles, category, target_category):\n",
    "    sub_articles = articles[category==target_category]\n",
    "    print len(sub_articles)\n",
    "    assigned_articles = print_topic_descriptions(sub_articles)\n",
    "    return sub_articles, assigned_articles\n",
    "\n",
    "sub_articles, article_categories = sub_topics(doc_bodies, assigned_cluster, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-92bbc38a8b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-392aaac445eb>\u001b[0m in \u001b[0;36msub_topics\u001b[0;34m(articles, category, target_category)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msub_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0massigned_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_topic_descriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msub_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned_articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-97366c28c8be>\u001b[0m in \u001b[0;36mprint_topic_descriptions\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msub_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massigned_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m    723\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         self.cluster_centers_, self.labels_, self.inertia_ = k_means(\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36m_check_fit_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n\u001b[0;32m--> 693\u001b[0;31m                 X.shape[0], self.n_clusters))\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=5"
     ]
    }
   ],
   "source": [
    "sub_articles, article_categories = sub_topics(sub_articles, article_categories, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "0: subsidies, fossil, fuel, countries, egypt, government, international, iea, reforms, prices\n",
      "1: oil, prices, price, saudi, production, opec, shale, falling, states, arabia\n",
      "2: prices, gasoline, gas, oil, gallon, suvs, driving, efficient, vehicles, consumers\n",
      "3: pipeline, oil, keystone, sands, xl, alberta, obama, rail, canada, project\n",
      "4: oil, gas, imports, energy, coal, power, shell, drilling, states, middle\n"
     ]
    }
   ],
   "source": [
    "sub_articles, article_categories = sub_topics(sub_articles, article_categories, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
